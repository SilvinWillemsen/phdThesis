\chapter{Analysis Techniques}\label{ch:analysis}

This chapter provides some techniques to analyses FD schemes .

Techniques to analyse PDEs also exist, but the focus here is on the discrete schemes.

\section{Preamble}

% \subsection{General Mathematical Tools}

Before going into analysis techniques of the FD schemes presented in this chapter, some mathematical tools used for these techniques will be presented. 

\subsubsection{Continuous time}
For two functions $f(x)$ and $g(x)$ defined for $x\in\D$, their $l_2$ inner product and $l_2$ norm are defined as
\begin{equation}\label{eq:contInnerProd}
    \langle f, g\rangle_\D = \int_\D fg dx \quad \text{and} \quad \lVert f \rVert_\D = \sqrt{\langle f, f \rangle_\D}
\end{equation}

\subsubsection{Integration by parts}


\subsubsection{Discrete-time}
Inner product of any time series $f^n$ and $g^n$ defined over domain $\D$ and the discrete counterpart to \eqref{eq:contInnerProd} is
\begin{equation}\label{eq:discInnerProd}
    \langle f^n, g^n \rangle_\D = \sum_{l\in\D} h f_l^n g_l^n
\end{equation}
where the multiplication by $h$ is the discrete counterpart of $dx$ the continuous definition. 

\subsection{Summation by Parts}
for 

\section{Matrices}
For several purposes, such as implementation in \texttt{MATLAB} and several analysis techniques described shortly, is useful to write an FD scheme in \textit{matrix form}. 

In this document, matrices and vectors are written using bold symbols. \SWcomment[Many notations exist, blabla $\bar a$ $\vec a$] A matrix uses a capital letter whereas vectors are decapitalised. The dimensions of a matrix are denoted using ``$row \times column$''. A $6\times 14$ matrix, for example, thus has $6$ rows and $14$ columns. Along those lines, a \textit{row vector} is a matrix with $1$ row and more than $1$ column and a \textit{column vector} is a matrix with $1$ column and more than $1$ row. \SWcomment[If a matrix has only $1$ row and $1$ column, it can be used as a \textit{scalar}.]

\subsubsection{Matrix Multiplication}
In order for matrix multiplication (including matrix-vector multiplication) to be valid, the number of columns of the first matrix needs to be equal to the number of rows in the second matrix. The result will then be a matrix with a number of rows equal to that of the first matrix and a number of columns equal to that of the second matrix. See Figure \ref{fig:matrixVector} for reference.

As an example, consider the $L
\times M$ matrix $
\A$ and a $M\times N$ matrix $
\B$. The multiplication $\A\B$ is defined as the number of columns of matrix $
\A$ ($M$) is equal to the number of rows of matrix $\B$ (also $M$). The result, $\C$, is a $L \times N$ matrix. The multiplication $\B\A$ is undefined as the number of columns of the first matrix does not match the number of rows in the second matrix. 

Multiplying two matrices written in their dimensions as 
\begin{equation}
    \overbrace{(L\times M)}^{\A}\cdot \overbrace{(M\times N)}^{\B} = \overbrace{(L
    \times N)}^{\C}
\end{equation}
 
\begin{figure}[h]
    \includegraphics[width=\textwidth]{figures/analysis/matrixVector.pdf}
    \caption{Visualisation of valid matrix multiplications. The ``inner'' dimensions (columns of the left matrix and rows of the right) must match and result in a matrix with a size of ``outer'' dimensions (rows of the left matrix and columns of the right). \label{fig:matrixVector}}
\end{figure}

\subsubsection{In a FDTD context}
Matrix multiplication when working with FDTD methods usually involves multiplying a square matrix (with equal rows and columns) onto a column vector (see Figure \ref{fig:matrixVector}a). Consider a square matrix with $(N+1)\times (N+1)$ elements $\A$ and a $(N+1) \times 1$ column vector $\u$. Multiplying these results in a $(N+1) \times 1$ column vector $\w$:
\begin{equation}
    \A\u = \w.
\end{equation}
Expanding this operation results in 
\begin{equation}
    \underbrace{\begin{bmatrix}
        a_{00} & a_{01} & \hdots & a_{0N}\\
        a_{10} & a_{11} & \hdots & a_{2N}\\
        \vdots & \vdots & &\vdots\\
        a_{N0} & a_{N1} & \hdots & a_{NN}
    \end{bmatrix}}_{\A}
    \underbrace{\begin{bmatrix}
        u_0\\
        u_1\\
        \vdots\\
        u_N
    \end{bmatrix}}_{\u} = 
    \underbrace{\begin{bmatrix}
        a_{00}u_0 + a_{01}u_1 + \hdots + a_{0N}u_N\\
        a_{10}u_0 + a_{11}u_1 + \hdots + a_{1N}u_N\\
        \vdots\\
        a_{N0}u_0 + a_{N1}u_1 + \hdots + a_{NN}u_N
    \end{bmatrix}}_{\w}
\end{equation}
where the subscripts for $a$ indicate the indices for the row and column.

\subsubsection{Operators in Matrix Form}
FD operators approximating first-order spatial derivatives can be written in matrix form and applied to a column vector $\u$ containing the state of the system. The size of these matrices depend on the number of grid points the system is described for and the boundary conditions. 

Not assuming any size for now, the FD operators in \eqref{eq:discFirstSpace} can be written in matrix form according to
\begin{gather*}\small
    \mathbf{D}_{x+} = \frac{1}{h}\begin{bmatrix}
        \ddots &\ddots & & & \mathbf{0}&\\
         & -1 & 1 & & & \\
        & & -1 & 1 & & \\
        & & & -1 & 1 & \\
        & & & & -1 & \ddots\\
        &\mathbf{0} & & & & \ddots \\
    \end{bmatrix}
    \quad
    \mathbf{D}_{x-} = \frac{1}{h}\begin{bmatrix}
        \ddots & & & & \mathbf{0}&\\
        \ddots & 1 & & & & \\
        & -1 & 1 & & & \\
        & & -1 & 1 & & \\
        & & & -1 & 1 & \\
        &\mathbf{0} & & & \ddots & \ddots \\
    \end{bmatrix}\\
    \\
    \mathbf{D}_{x\cdot} = \frac{1}{2h}\begin{bmatrix}\small
        \ddots &\ddots & & & \mathbf{0}&\\
        \ddots & 0 & 1 & & & \\
        & -1 & 0 & 1 & & \\
        & & -1 & 0 & 1 & \\
        & & & -1 & 0 & \ddots \\
        &\mathbf{0} & & & \ddots & \ddots \\
    \end{bmatrix}\\
\end{gather*}
%
Averaging operators $\mxp$, $\mxm$ and $\mxd$ are defined in a similar way:

\begin{gather*}
    \mathbf{M}_{x+} = \frac{1}{2}\begin{bmatrix}
        \ddots &\ddots & & & \mathbf{0}&\\
         & 1 & 1 & & & \\
        & & 1 & 1 & & \\
        & & & 1 & 1 & \\
        & & & & 1 & \ddots\\
        &\mathbf{0} & & & & \ddots \\
    \end{bmatrix}
    \qquad
    \mathbf{M}_{x-} = \frac{1}{2}\begin{bmatrix}
        \ddots & & & & \mathbf{0}&\\
        \ddots & 1 & & & & \\
        & 1 & 1 & & & \\
        & & 1 & 1 & & \\
        & & & 1 & 1 & \\
        &\mathbf{0} & & & \ddots & \ddots \\
    \end{bmatrix}\\
    \\
    \mathbf{M}_{x\cdot} = \frac{1}{2}\begin{bmatrix}
        \ddots &\ddots & & & \mathbf{0}&\\
        \ddots & 0 & 1 & & & \\
        & 1 & 0 & 1 & & \\
        & & 1 & 0 & 1 & \\
        & & & 1 & 0 & \ddots \\
        &\mathbf{0} & & & \ddots & \ddots \\
    \end{bmatrix}\\
\end{gather*}

It is important to notice that only spatial operators are written in this matrix form and then applied to state vectors at different time steps ($n+1$, $n$ and $n-1$). 

Finally, the identity matrix is a matrix with only $1$s on the diagonal and $0$s elsewhere as
\begin{equation}
    \I = \begin{bmatrix}
        \ddots & & & & \mathbf{0}&\\
         & 1 & & & & \\
        & & 1 & & & \\
        & & & 1 & & \\
        & & & & 1 & \\
        &\mathbf{0} & & &  & \ddots \\
    \end{bmatrix}\\
\end{equation}


\subsubsection{Schemes and Update Equations in Matrix Form}
With these building blocks, Eq. \eqref{eq:1DwaveFDS} can be written in matrix form.

If Dirichlet boundary conditions are used (Eq. \eqref{eq:discreteDirichlet}) the end points of the system do not have to be included in the calculation. The values of the grid function $\uln$ for $l\in \{1, \hdots, N-1\}$ can be stored in a state vector according to $\u^n = [u_1^n, \hdots, u_{N-1}^n]^T$ where $T$ denotes the transpose operation\todo{check whether to not put this earlier}. Furthermore, $(N-1) \times (N-1)$ matrices $\mathbf{D}_{x+}$ and $\mathbf{D}_{x-}$ can be multiplied to get a same-sized matrix $\Dxx$:
\begin{equation}
    \Dxx = \mathbf{D}_{x+}\mathbf{D}_{x-} = \frac{1}{h^2}\begin{bmatrix}
        -2 & 1 & & &\mathbf{0}\\
        1 & -2 & 1 & & \\
        & \ddots & \ddots & \ddots & \\
        & & 1 & -2 & 1 \\
        \mathbf{0}& & & 1 & -2 
    \end{bmatrix}.
\end{equation}

If instead, Neumann boundary conditions are used (Eq. \eqref{eq:discreteDirichlet}), the values of $\uln$ for the full range $l\in \{0, \hdots, N\}$ will be stored as $\u^n=[u_0^n, \hdots, u_N^n]^T$ and the $(N+1) \times (N+1)$ matrix $\Dxx$ will be 
\begin{equation}
    \Dxx = \frac{1}{h^2}\begin{bmatrix}
        -2 & 2 & & &\mathbf{0}\\
        1 & -2 & 1 & & \\
        & \ddots & \ddots & \ddots & \\
        & & 1 & -2 & 1 \\
        \mathbf{0}& & & 2 & -2 
    \end{bmatrix},
\end{equation}
where the $2$s correspond to the multiplication by $2$ with $u_1^n$ and $u_{N-1}^n$ in Eqs. \eqref{eq:1DWaveLeftBound} and \eqref{eq:1DWaveRightBound} respectively.

Regardless of the boundary conditions, the FD scheme in \eqref{eq:1DwaveFDS} can be written in matrix form as
\begin{equation}\label{eq:1DwaveMatrix}
    \frac{1}{k^2}\left(\u^{n+1} - 2 \u + \u^{n-1}\right) = c^2 \Dxx \u^n,
\end{equation}
and rewritten to a matrix form of the update equation analogous to Eq. \eqref{eq:1DwaveUpdate}
\begin{equation}
    \u^{n+1} = (2\I + c^2k^2 \Dxx )\u^n - \u^{n-1}.
\end{equation}
The identity matrix is necessary here for correct matrix addition.

\subsection{Note on square matrices}
If a matrix is square it has the potential to have an inverse. A square matrix $\A$ is invertable if there exists a matrix $\B$ such that
\begin{equation}
    \A \B = \B \A = \I. 
\end{equation}
This matrix $\B$ is then called the \textit{inverse} of $\A$ and written as $\A^{-1}$.
That a matrix is square does not mean that it has an inverse, and is called \textit{singular}. 


\subsection{System of Linear Equations}\label{sec:linearEquations}
A number of unknowns described by the same number of equations can be solved using a 

\begin{align*}
    \A \u &= \w\\
    \u &= \A^{-1}\w
\end{align*}

\subsection{Eigenvalue Problems}\label{sec:eigenValueProblems}
A square matrix $\A$ is characterised by its \textit{eigenvalues} and corresponding \textit{eigenvectors}. In a FDTD context, these are usually associated with the modes of a system, where the eigenvalues relate to the modal frequencies and \todo{ask stefan} the eigenvectors the the modal shapes. Section \ref{sec:modalAnalysis} will provide more information on this.

To find these characteristic values for a $p\times p$ matrix $\A$ an equation of the following form must be solved 
\begin{equation}
    \A \boldPhi = \lambda \boldPhi.
\end{equation}
This is called is an \textit{eigenvalue problem} and has $p$ solutions (corresponding to the dimensions of $\A$). These are the $p$\th eigenvector $\boldPhi_p$ and the corresponding eigenvalue $\lambda_p$ calculated using
\begin{equation}
    \lambda_p = \eig_p(\A),
\end{equation}
where $\eig_p(\cdot)$ denotes the $p$\th eigenvalue of. Instead of delving too deep into eigenvalue problems and the process of how to solve them, an easy way to obtain the solutions using \texttt{MATLAB} is provided here:

\begin{center}\todo{Check code here!}
    \texttt{[\textasciitilde, lambda, phi] = eig(A, {\color[HTML]{A100F4}'vector'});}
\end{center}
The eigenvalues are given in a $p \times 1$ column vector \texttt{lambda} and the $p$\th eigenvector appears in the $p$\th column of $p\times p$ matrix \texttt{phi}. Note that the outcome is not sorted! To do this, do 
%
\begin{center}
    \begin{tabular}{l}
    \texttt{[lambda, order] = sort(lambda);}\\
    \texttt{phi = phi(:, order);}
    \end{tabular}
\end{center}

\section{von Neumann Analysis}\label{sec:stabilityAnalysis}
Much literature gives the stability condition using an ``it can be shown that'' argument (fx. \cite{Bilbao2009}). Here, I would like to take the opportunity to 

Finding stability conditions for models

The following trigonometric identities will come in handy when performing the analyses \cite[p. 71]{Abramowitz1972}:
\begin{subequations}
    \begin{gather}
        \sin(x) = \frac{e^{jx} - e^{-jx}}{2j}\ \ \Rightarrow \ \ \sin^2(x) %= \frac{e^{j2x} - 2e^{jx-jx}+ e^{-j2x}}{-4} 
        = \frac{e^{j2x} + e^{-j2x}}{-4} + \frac{1}{2},\label{eq:sinIdentity}\\
        \cos(x) = \frac{e^{jx} + e^{-jx}}{2}\ \ \Rightarrow \ \ \cos^2(x) %= \frac{e^{j2x} + 2e^{jx-jx}+ e^{-j2x}}{4} 
        = \frac{e^{j2x} + e^{-j2x}}{4} + \frac{1}{2}.\label{eq:cosIdentity}
    \end{gather}
\end{subequations}

One can analyse a \todo{FULL DOC SWEEP:check `a' or `an' FD scheme}FD scheme by 
\begin{equation}
    u_l^n = z^n e^{jl\beta h}
\end{equation}
where $\beta$ is a real wavenumber. Important to remember is that without a shift in space (fx. $l+1$) or time (fx. $n-1$) $l = 0$ or $n=0$ respectively:
\begin{subequations} \label{eq:identitiesZ}
    \begin{align}
        u_l^n &= z^0 e^{j0\beta h} = 1\\
        u_{l+1}^n &= z^0 e^{j1\beta h} = e^{j\beta h}\\
        u_{l-1}^n &= z^0 e^{j(-1)\beta h} = e^{-j\beta h}\\
        u_{l+2}^n &= z^0 e^{j2\beta h} = e^{j\beta h}\\
        u_{l-2}^n &= z^0 e^{j(-2)\beta h}= e^{-j2\beta h}\\
        u_l^{n+1}&= z^1 e^{j0\beta h} = z\\
        u_l^{n-1}&= z^{-1} e^{j0\beta h} = z^{-1}
    \end{align}
\end{subequations}

\todo{Talk about solution of }

\section{Energy Analysis}\label{sec:energyAnalysis}

The Hamiltonian or $\mathfrak{H}$

Multiplying scheme by $\left(\dtd \uln \right)$

Debugging physical models...
One can plot the energy of the system and in a lossless system, the rate of change of the total energy should be 0, i.e.,
\begin{equation}\label{eq:unchangedEnergy}
    \delta_{t+}\mathfrak{h} = 0 \quad \Longrightarrow \quad \mathfrak{h}^n = \mathfrak{h}^0.
\end{equation}
.

Although the energy of a lossless system should be unchanged according to Eq. \eqref{eq:unchangedEnergy}, but in a finite precision simulation, ultra slight fluctuations of the energy should be visible due to rounding errors. 

Plotting energy should be within \textit{machine precision}, which mostly is in the range of $10^{-15}$

\subsubsection{Product Identities}
Some useful identities used in this work are
\begin{subequations}
    \begin{align}
        (\dtd \uln)(\dtt \uln) &= \dtp \left(\frac{1}{2}(\dtm \uln)^2\right),\label{eq:prodIdentity1}\\
        (\dtd \uln)\uln &= \dtp \left(\frac{1}{2}\uln e_{t-}\uln\right),\label{eq:prodIdentity2}\\
        (\dtp \uln)(\mtp \uln) &= \dtp \left(\frac{1}{2}(\uln)^2\right),\label{eq:prodIdentity3}\\
        (\dtd \uln)(\mu_{t\cdot}\uln) &= \dtd\left(\frac{1}{2} (\uln)^2\right).\label{eq:prodIdentity4}
    \end{align}
\end{subequations}
Again, these can be used for spatial derivatives as well by substituting the `$t$' subscripts for `$x$'. Also  to 

When an operator is applied to a product of two grid functions, the discrete counter part of the product rule needs to be used according to
\begin{equation}
    \dtp (\uln\wln) = (\dtp \uln)(\mtp\wln) + (\mtp \uln)(\dtp \wln).
\end{equation}


\subsection{Stability Analysis}
One can perform stability analysis using the energy analysis techniques presented here. To arrive at a condition the energy must be \textit{positive definite} 

\section{Modal Analysis}
\label{sec:modalAnalysis}
Modes are the resonant frequencies of a system. The amount of modes that a discrete system contains depends on the amount of moving points. A mass-spring system thus has one resonating mode, but a FD scheme of the 1D wave equation with $N = 30$ and Dirichlet boundary conditions will have $29$ modes. This section will show how to obtain the modes of an FD scheme implementing the 1D wave equation. 

We start by using the the matrix form of FD scheme \eqref{eq:1DwaveFDS} from Eq. \eqref{eq:1DwaveMatrix}
\begin{equation*}
    \frac{1}{k^2}\left(\u^{n+1}-2\u^n+\u^{n-1}\right) = c^2 \Dxx\u.
\end{equation*}
Following \cite{theBible} we assume a solution of the form $\u = z^n\boldPhi$ \todo{more explanation, perhaps refer to von neumann analysis in \ref{sec:stabilityAnalysis}}. Substituting this into the above equation yields the characteristic equation
\begin{equation}
    (z - 2 + z^{-1})\boldPhi = c^2k^2\Dxx \boldPhi.
\end{equation}
This is an eigenvalue problem (see Section \ref{sec:eigenValueProblems}) where the $p$\th solution $\boldPhi_p$ may be interpreted as the modal shape of mode $p$. The modal frequencies are the solutions to the following equations:
\begin{gather}
    z_p-2+z_p^{-1} = c^2k^2\text{eig}_p(\Dxx),\nonumber\\
    z_p+(-2-c^2k^2\text{eig}_p(\Dxx))+z_p^{-1}=0.
\end{gather}
\SWcomment[If the CFL condition for the scheme is satisfied, the roots will lie on the unit circle.] Furthermore we can substitute a test solution $z_p=e^{j\omega_pk}$ with angular frequency of the $p$\th mode $\omega_p$ and solve for these:
\begin{align*}
    e^{j\omega_pk}+e^{-j\omega_pk}-2-c^2k^2\text{eig}_p(\Dxx)&=0,\\
    \frac{e^{j\omega_pk}+e^{-j\omega_pk}}{-4}+\frac{1}{2}+\frac{c^2k^2}{4}\text{eig}_p(\Dxx)&=0,
\end{align*}
and using Eq. \eqref{eq:sinIdentity} we get
\begin{align}
    \sin^2(\omega_pk/2)&+\frac{c^2k^2}{4}\text{eig}_p(\Dxx)=0,\nonumber\\
    \sin(\omega_pk/2)&=\frac{ck}{2}\sqrt{-\text{eig}_p(\Dxx)},\nonumber\\
    \omega_p &= \frac{2}{k}\sin^{-1}\left(\frac{ck}{2}\sqrt{-\text{eig}_p(\Dxx)}\right).
\end{align}

\subsection{One-Step Form}
For more complicated systems, where the coefficients of $z$ and $z^{-1}$ in the characteristic equation are not identical for example, it is useful to rewrite the update in \textit{one-step form}. Although the eigenfrequency calculation needs to be done on a larger matrix, it allows for a more general and direct way to calculate the modal frequencies and damping coefficients per mode. 

If matrix $\A$ has an inverse, any scheme of the form
\begin{equation}
    \A\u^{n+1}=\B\u^n + \C\u^{n-1},
\end{equation}
can be rewritten to
\begin{equation}\label{eq:oneStepForm}
    \underbrace{\begin{bmatrix}
        \u^{n+1}\\
        \u^n
    \end{bmatrix}}_{\w^{n+1}} = 
    \underbrace{\begin{bmatrix}
        \A^{-1}\B & \A^{-1}\C\\
        \I & \mathbf{0}
    \end{bmatrix}}_{\Q}
    \underbrace{\begin{bmatrix}
        \u^n\\
        \u^{n-1}
    \end{bmatrix}}_{\w^n}
\end{equation}
which relates the unknown state of the system to the known state through matrix $\Q$ which encompases the scheme. The sizes of the identity matrix $\I$ and zero matrix $\mathbf{0}$ are the same size as $\A, \B$ and $\C$ and are essentially used to make the matrix square and allow for eigenvalue calculation.

Again we can assume solutions of the form $\w = z^n\boldPhi$ \SWcomment[(where $\boldPhi$ is less-trivially connected to the modal shapes)] and get
\begin{equation}
    z\boldPhi = \Q\boldPhi ,
\end{equation}
which can be solved for the $p$th eigenvalue as
\begin{equation}
    z_p = \text{eig}_p(\Q).
\end{equation}
As the scheme could exhibit damping, the test solution needs to include this and will be $z_p = e^{s_pk}$ with complex frequency $s_p= j\omega_p + \sigma_p$ and damping of the $p$th eigenfrequency $\sigma_p$. Substituting the test solution and solving for $s_p$ yields
\begin{align}
    e^{s_pk} &= \text{eig}_p(\Q),\nonumber\\
    s_p &= \frac{1}{k}\ln \left(\text{eig}_p(\Q)\right).\label{eq:sp}
\end{align}
Solutions for the frequency and damping for the $p$th eigenvalue can then be obtained through
\begin{equation}
    \omega_p = \mathfrak{I}(s_p) \quad \text{and} \quad \sigma_p = \mathfrak{R}(s_p),
\end{equation}
where $\mathfrak{I}(\cdot)$ and $\mathfrak{R}(\cdot)$ denote the ``imaginary part of'' and ``real part of'' respectively. 

As the elements of $\Q$ are real-valued, the solutions $s_p$ in Eq. \eqref{eq:sp} come in complex conjugates. For analysis, only the $\mathfrak{I}(s_p)\geq 0$ should be considered as these correspond to non-negative frequencies.

\section{Dispersion analysis}\label{sec:dispersionAnalysis}
