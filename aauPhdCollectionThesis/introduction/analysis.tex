\chapter{Analysis Techniques}\label{ch:analysis}

This chapter provides some techniques to analyses FD schemes .

Techniques to analyse PDEs also exist, but the focus here is on the discrete schemes.

The analysis techniques will be especially explained from a practical side, rather than providing all the theory that they are based on. Obviously, references to work substantiating the claims made in this chapter will be provided. \todo{probably not necessary}

References to the origins of these techniques will be provided for the interested reader. 

\section{Matrices}
For several purposes, such as implementation in \texttt{MATLAB} and several analysis techniques described shortly, is useful to write a FD scheme in \textit{matrix form}. A matrix is a rectangular array with numerical elements and its dimensions are denoted using ``$row \times column$''. A $3\times 5$ matrix, for example, thus has $3$ rows and $5$ columns (see Figure \ref{fig:matrixA}). Along those lines, a \textit{row vector} is a matrix with $1$ row and more than $1$ column and a \textit{column vector} is a matrix with $1$ column and more than $1$ row. \SWcomment[If a matrix has only $1$ row and $1$ column, it can be used as a \textit{scalar}.] 

In this document, matrices and vectors are written using bold symbols. \SWcomment[Many notations exist, blabla $\bar a$ $\vec a$] A matrix is denoted by a capital letter -- such as $\A$ -- whereas vectors are decapitalised -- such as $\u$. An element in a matrix is denoted with a non-bold, decapitalised variable, where the subscripts indicate the indices of the row and column. For example, the element in the 2\textsuperscript{nd} row and the 4\th column of a matrix $\A$ is denoted as $a_{24}$.

A matrix or vector can be \textit{transposed}, and is indicated with the $T$ operator. Transposing a matrix $\A$ is denoted by $\A^T$.  means that the elements in the $i$\th row and the $j$\th column of the original matrix become the elements in the $j$\th row and the $i$\th column of the transposed matrix. Essentially the row and column indices of the elements inside the matrix get switched according to
\begin{equation}\label{eq:matrixTransposition}
    a_{ij} = a_{ji}.
\end{equation} 
Also see Figure \ref{fig:matrixTransp}. For a row vector, the transpose operation simply changes it to a column vector and vice versa. Another way of seeing a transpose is that all the elements get flipped over the \textit{main diagonal} of the matrix. The main diagonal comprises the elements $a_{ij}$ where $i=j$ and a transpose does not affect the location of these elements. 

\begin{figure}[h]
    \centering
    \subfloat[A $3\times 5$ matrix $\A$.\label{fig:matrixA}]{\includegraphics[width=0.45\textwidth]{figures/analysis/matrixA1.pdf}}\hspace{0.03cm}
    \subfloat[A transposed matrix $\A^T$ of size $5\times 3$.\label{fig:matrixAT}]{\includegraphics[width=0.45\textwidth]{figures/analysis/matrixAT1.pdf}}
    \caption{A matrix $\A$ and its transpose $\A^T$. The elements get flipped along the main diagonal of the matrix according to Eq. \eqref{eq:matrixTransposition}. \label{fig:matrixTransp}}
\end{figure}

\subsubsection{Matrix Multiplication}
In order for matrix multiplication (this includes matrix-vector multiplication) to be valid, the number of columns of the first matrix needs to be equal to the number of rows in the second matrix. The result will then be a matrix with a number of rows equal to that of the first matrix and a number of columns equal to that of the second matrix. See Figure \ref{fig:matrixVector} for reference.

As an example, consider the $L\times M$ matrix $
\A$ and a $M\times N$ matrix $\B$ with $L\neq N$. The multiplication $\A\B$ is defined as the number of columns of matrix $\A$ ($M$) is equal to the number of rows of matrix $\B$ (also $M$). The result, $\C$, is a $L \times N$ matrix. The multiplication $\B\A$ is undefined as the number of columns of the first matrix does not match the number of rows in the second matrix. A valid multiplication of two matrices written in their dimensions is
\begin{equation}
    \overbrace{(L\times M)}^{\A}\cdot \overbrace{(M\times N)}^{\B} = \overbrace{(L
    \times N)}^{\C}.
\end{equation}

A matrix can always be multiplied by a scalar (single number), which simply multiplies every element of the matrix by the scalar. 
\begin{figure}[h]
    \includegraphics[width=\textwidth]{figures/analysis/matrixVector.pdf}
    \caption{Visualisation of valid matrix multiplications. The ``inner'' dimensions (columns of the left matrix and rows of the right) must match and result in a matrix with a size of ``outer'' dimensions (rows of the left matrix and columns of the right). \label{fig:matrixVector}}
\end{figure}

\subsection{In a FDTD context}
Matrix multiplication when working with FDTD methods usually involves multiplying a square matrix (with equal rows and columns) onto a column vector (see Figure \ref{fig:matrixVector}a). Consider a $(N+1)\times (N+1)$ square matrix $\A$ and a $(N+1) \times 1$ column vector $\u$. Multiplying these results in a $(N+1) \times 1$ column vector $\w$:
\begin{equation}
    \A\u = \w.
\end{equation}
Expanding this operation results in
\begin{equation}
    \underbrace{\begin{bmatrix}
        a_{00} & a_{01} & \hdots & a_{0N}\\
        a_{10} & a_{11} & \hdots & a_{2N}\\
        \vdots & \vdots & &\vdots\\
        a_{N0} & a_{N1} & \hdots & a_{NN}
    \end{bmatrix}}_{\A}
    \underbrace{\begin{bmatrix}
        u_0\\
        u_1\\
        \vdots\\
        u_N
    \end{bmatrix}}_{\u} = 
    \underbrace{\begin{bmatrix}
        a_{00}u_0 + a_{01}u_1 + \hdots + a_{0N}u_N\\
        a_{10}u_0 + a_{11}u_1 + \hdots + a_{1N}u_N\\
        \vdots\\
        a_{N0}u_0 + a_{N1}u_1 + \hdots + a_{NN}u_N
    \end{bmatrix}}_{\w}
\end{equation}
where the indexing starts at $0$ rather than $1$ here, as that relates better to a FDTD context.

\subsubsection{Operators in Matrix Form}
FD operators approximating spatial derivatives and averages can be written in matrix form and applied to a column vector $\u$ containing the state of the system at time index $n$. These matrices are square and their sizes depend on the number of grid points the system is described for and the boundary conditions. Not assuming a specific size for now, the FD operators in \eqref{eq:discFirstSpace} can be written in matrix form according to
% \setstackgap{L}{1.1\baselineskip}
% \fixTABwidth{T}
\setstackgap{L}{14pt}
\setstacktabbedgap{4pt}
\def\lrgap{\kern3pt}
\fixTABwidth{T}

\def\xbracketMatrixstack#1{\left[\lrgap\tabbedCenterstack{#1}\lrgap\right]}

\begin{equation*}
    \mathbf{D}_{x+} = \frac{1}{h}\xbracketMatrixstack{
        \ddots &\ddots & & & \mathbf{0}&\\
         & -1 & 1 & & & \\
        & & -1 & 1 & & \\
        & & & -1 & 1 & \\
        & & & & -1 & \ddots\\
        &\mathbf{0} & & & & \ddots
    }\qquad \mathbf{D}_{x-} = \frac{1}{h}\xbracketMatrixstack{
        \ddots & & & & \mathbf{0}&\\
        \ddots & 1 & & & & \\
        & -1 & 1 & & & \\
        & & -1 & 1 & & \\
        & & & -1 & 1 & \\
        &\mathbf{0} & & & \ddots & \ddots
    }
\end{equation*}

\begin{equation*}
    \mathbf{D}_{x\cdot} = \frac{1}{2h}\xbracketMatrixstack{
        \ddots &\ddots & & & \mathbf{0}&\\
        \ddots & 0 & 1 & & & \\
        & -1 & 0 & 1 & & \\
        & & -1 & 0 & 1 & \\
        & & & -1 & 0 & \ddots \\
        &\mathbf{0} & & & \ddots & \ddots
    }
\end{equation*}
% \begin{gather*}
%     \mathbf{D}_{x+} = \frac{1}{h}
%     \begin{bmatrix}
%         \ddots &\ddots & & & \mathbf{0}&\\
%          & -1 & 1 & & & \\
%         & & -1 & 1 & & \\
%         & & & -1 & 1 & \\
%         & & & & -1 & \ddots\\
%         &\mathbf{0} & & & & \ddots \\
%     \end{bmatrix}
%     \quad
%     \mathbf{D}_{x-} = \frac{1}{h}\begin{bmatrix}
%         \ddots & & & & \mathbf{0}&\\
%         \ddots & 1 & & & & \\
%         & -1 & 1 & & & \\
%         & & -1 & 1 & & \\
%         & & & -1 & 1 & \\
%         &\mathbf{0} & & & \ddots & \ddots \\
%     \end{bmatrix}\\
%     \\
%     \mathbf{D}_{x\cdot} = \frac{1}{2h}\begin{bmatrix}
%         \ddots &\ddots & & & \mathbf{0}&\\
%         \ddots & 0 & 1 & & & \\
%         & -1 & 0 & 1 & & \\
%         & & -1 & 0 & 1 & \\
%         & & & -1 & 0 & \ddots \\
%         &\mathbf{0} & & & \ddots & \ddots \\
%     \end{bmatrix}\\
% \end{gather*}
%
where the diagonal dots denote that the values on the respective diagonals continue until the top-left and bottom-right corners of the matrix. \todo{is this how you explain it?} The $\boldsymbol{0}$s indicate that the rest of the values in the matrix are zeros.

Averaging operators $\mxp$, $\mxm$ and $\mxd$ are defined in a similar way:

\begin{equation*}
    \mathbf{M}_{x+} = \frac{1}{2}\xbracketMatrixstack{
        \ddots &\ddots & & & \mathbf{0}&\\
         & 1 & 1 & & & \\
        & & 1 & 1 & & \\
        & & & 1 & 1 & \\
        & & & & 1 & \ddots\\
        &\mathbf{0} & & & & \ddots
    }
    \qquad
    \mathbf{M}_{x-} = \frac{1}{2}\xbracketMatrixstack{
        \ddots & & & & \mathbf{0}&\\
        \ddots & 1 & & & & \\
        & 1 & 1 & & & \\
        & & 1 & 1 & & \\
        & & & 1 & 1 & \\
        &\mathbf{0} & & & \ddots & \ddots
    }
\end{equation*}

\begin{equation*}    
    \mathbf{M}_{x\cdot} = \frac{1}{2}\xbracketMatrixstack{
        \ddots &\ddots & & & \mathbf{0}&\\
        \ddots & 0 & 1 & & & \\
        & 1 & 0 & 1 & & \\
        & & 1 & 0 & 1 & \\
        & & & 1 & 0 & \ddots \\
        &\mathbf{0} & & & \ddots & \ddots
    }
\end{equation*}

It is important to notice that only spatial operators are written in this matrix form and then applied to state vectors at different time steps ($\u^{n+1}$, $\u^n$ and $\u^{n-1}$). 

Finally, the identity matrix is a matrix with only $1$s on the diagonal and $0$s elsewhere as
\begin{equation*}
    \I = \xbracketMatrixstack{
        \ddots & & & & \mathbf{0}&\\
         & 1 & & & & \\
        & & 1 & & & \\
        & & & 1 & & \\
        & & & & 1 & \\
        &\mathbf{0} & & &  & \ddots
    }
\end{equation*}


\subsubsection{Schemes and Update Equations in Matrix Form}
With the spatial operators in matrix form presented above, the FD scheme of the 1D wave equation in \eqref{eq:1DwaveFDS} can be written in matrix form.

If the Dirichlet boundary conditions in \eqref{eq:discreteDirichlet} are used, the end points of the system do not have to be included in the calculation. The values of the grid function $\uln$ for $l\in \{1, \hdots, N-1\}$ can then be stored in a column vector according to $\u^n = [u_1^n, \hdots, u_{N-1}^n]^T$. Furthermore, $(N-1) \times (N-1)$ matrix $\Dxx$ is defined as
\begin{equation}\label{eq:DxxDef}
    \Dxx = \mathbf{D}_{x+} - \mathbf{D}_{x-} = \frac{1}{h^2}\xbracketMatrixstack{
        -2 & 1 & & &\mathbf{0}\\
        1 & -2 & 1 & & \\
        & \ddots & \ddots & \ddots & \\
        & & 1 & -2 & 1 \\
        \mathbf{0}& & & 1 & -2 
    }.
\end{equation}

If instead, Neumann boundary conditions in Eq. \eqref{eq:discreteDirichlet} are used, the values of $\uln$ for the full range $l\in \{0, \hdots, N\}$ need be stored as $\u^n=[u_0^n, \hdots, u_N^n]^T$ and the $(N+1) \times (N+1)$ matrix $\Dxx$ will be 
\begin{equation}\label{eq:DxxDefNeumann}
    \Dxx = \frac{1}{h^2}
    \xbracketMatrixstack{
        -2 & 2 & & &\mathbf{0}\\
        1 & -2 & 1 & & \\
        & \ddots & \ddots & \ddots & \\
        & & 1 & -2 & 1 \\
        \mathbf{0}& & & 2 & -2 
    },
\end{equation}
where the $2$s in the top and bottom row correspond to the multiplication by $2$ with $u_1^n$ and $u_{N-1}^n$ in Eqs. \eqref{eq:1DWaveLeftBound} and \eqref{eq:1DWaveRightBound} respectively.

Regardless of the boundary conditions, the FD scheme in \eqref{eq:1DwaveFDS} can be written in matrix form as
\begin{equation}\label{eq:1DwaveMatrix}
    \frac{1}{k^2}\left(\u^{n+1} - 2 \u + \u^{n-1}\right) = c^2 \Dxx \u^n,
\end{equation}
and rewritten to a matrix form of the update equation analogous to Eq. \eqref{eq:1DwaveUpdate}
\begin{equation}
    \u^{n+1} = (2\I + c^2k^2 \Dxx )\u^n - \u^{n-1}.
\end{equation}
The identity matrix is necessary here for correct matrix addition.

\subsection{Matrix Inverse}\label{sec:inverse}
If a matrix has the same number of rows as columns, it is called a \textit{square matrix}. Square matrices have special properties, one of which is that it (usually) can be \textit{inverted}. A square matrix $\A$ is invertable if there exists a matrix $\B$ such that
\begin{equation}
    \A \B = \B \A = \I. 
\end{equation}
This matrix $\B$ is then called the \textit{inverse} of $\A$ and can be written as $\A^{-1}$. Not all square matrices have an inverse. In this case, the matrix is called \textit{singular}. Rather than going through how to invert a matrix by hand, or determining whether it is singular, the following function in \texttt{MATLAB} will provide the inverse of a matrix \texttt{A}:
\begin{center}
    \texttt{A\_inverted = inv(A);}
\end{center}

The inverse of a \textit{diagonal matrix} (a matrix with non-zero elements on its main diagonal and the rest zeros) is obtained by replacing the diagonal elements by their reciprocal. So for a diagonal $3\times 3$ matrix, the following holds:
\begin{equation*}
    \xbracketMatrixstack{
        a_{11} & 0 & 0\\
        0 & a_{12} & 0\\
        0 & 0 & a_{33}
    }^{-1} =\quad 
    \xbracketMatrixstack{
        \frac{1}{a_{11}} & 0 & 0\\
        0 & \frac{1}{a_{12}} & 0\\
        0 & 0 & \frac{1}{a_{33}}
    }.
\end{equation*}

\subsection{Systems of Linear Equations}\label{sec:linearEquations}
Matrices can be conveniently used to solve \textit{systems of linear equations}, a set of linear equations containing the same set of variables. 

For example, take the system of linear equations
\begin{align*}
    x + z &= 6\\
    z - 3y &= 7\\
    2x + y + 3z &= 15
\end{align*}
with independent variables $x$, $y$ and $z$. The goal is to find a solution for these variables that satisfy all three equations. This system could be solved by hand using algebraic methods, but alternatively, the system can be written in matrix form:
\begin{equation}
    \A \u = \w.
\end{equation}
Here, column vector $\u$ contains the independent variables $x$, $y$, and $z$, matrix $\A$ contains the coefficients multiplied onto these variables and $\w$ contains the right-hand side, i.e., the coefficients not multiplied onto any of the variables:
\begin{equation*}
    \underbrace{\xbracketMatrixstack{
        1& 0& 1\\
        0& -3& 1\\
        2& 1& 3}}_{\A}
    \underbrace{\xbracketMatrixstack{
        x\\
        y\\
        z
    }}_{\u} = \underbrace{\xbracketMatrixstack{
        6\\
        7\\
        15
    }}_{\w}
\end{equation*}
We can then solve for $\u$ by taking the inverse of $\A$ (see Section \ref{sec:inverse}) and multiplying this onto $\w$
\begin{equation}
    \u = \A^{-1}\w.
\end{equation}
Generally, if X unknowns are described by X equations, the unknowns can be solved for using this method.

Solving a system of linear equations can be implemented in \texttt{MATLAB} by using the code given in Section \ref{sec:inverse} 
\begin{center}
    \texttt{u = inv(A) * w;}
\end{center}
or more compactly, by using the `\texttt{\textbackslash}' operator:
\begin{center}
    \texttt{u = A\textbackslash w;}
\end{center}

This method will come in handy in Section ...


\subsection{Eigenvalue Problems}\label{sec:eigenValueProblems}
A square matrix $\A$ is characterised by its \textit{eigenvalues} and corresponding \textit{eigenvectors}. In a FDTD context, these are usually associated with the modes of a system, where the eigenvalues relate to the modal frequencies and \todo{ask stefan} the eigenvectors to the modal shapes. Section \ref{sec:modalAnalysis} will provide more information on this.

To find these characteristic values for a $p\times p$ matrix $\A$, an equation of the following form must be solved 
\begin{equation}
    \A \boldPhi = \lambda \boldPhi.
\end{equation}
This is called is an \textit{eigenvalue problem} and has $p$ solutions (corresponding to the dimensions of $\A$). These are the $p$\th eigenvector $\boldPhi_p$ and the corresponding eigenvalue $\lambda_p$ which is calculated using
\begin{equation}
    \lambda_p = \eig_p(\A),
\end{equation}
where $\eig_p(\cdot)$ denotes the $p$\th eigenvalue of. Instead of delving too deep into eigenvalue problems and the process of how to solve them, an easy way to obtain the solutions using \texttt{MATLAB} is provided here:

\begin{center}\todo{Check code here!}
    \texttt{[phi, lambda] = eig(A, {\color[HTML]{A100F4}'vector'});}
\end{center}
The $p$\th eigenvector appears in the $p$\th column of $p\times p$ matrix \texttt{phi} and the eigenvalues are given in a $p \times 1$ column vector \texttt{lambda}. Note that the outcome is not sorted! To do this, do 
%
\todo{check whether this is necessary}
\begin{center}
    \begin{tabular}{l}
    \texttt{[lambda, order] = sort(lambda);}\\
    \texttt{phi = phi(:, order);}
    \end{tabular}
\end{center}

\section{Mathematical Tools and Product Identities}
Some useful mathematical tools used for the energy analysis techniques presented in Section \ref{sec:energyAnalysis} will be shown here. The tools shown here can be applied to 1D systems. Tools for 2D systems will be introduced in Chapter \ref{ch:2Dsyst}.

\subsection{Inner product}\label{sec:innerProduct}
For two functions $f(x,t)$ and $g(x,t)$ defined for $x\in\D$ where $\mathcal{D} = [0,L]$, their $l_2$ inner product and $l_2$ norm are defined as
\begin{equation}\label{eq:contInnerProd}
    \langle f, g\rangle_\D = \int_\D fg dx \quad \text{and} \quad \lVert f \rVert_\D = \sqrt{\langle f, f \rangle_\D}.
\end{equation}
These functions do not have to be time-dependent (i.e., they can also simply be $f(x)$ and $g(x)$), but as all functions used in this work are in fact time-dependent, this is left for coherence. It is also important to note that these functions do not have to be `isolated' state variables per se (such as $u(x,t)$ used in the previous chapter), but could also be a state variable with a derivative applied to it. 

The discrete inner product of any two (1D) functions $f_l^n$ and $g_l^n$ defined for $l \in d$, with discrete domain $d = \{0,\hdots,N\}$, is
\begin{equation}\label{eq:discInnerProd}
    \langle f_l^n, g_l^n \rangle_d = \sum_{l = 0}^N h f_l^n g_l^n,
\end{equation}
where the multiplication by $h$ is the discrete counterpart of $dx$ in the continuous definition in \eqref{eq:contInnerProd}. 

Also useful are the primed inner product
\begin{equation}\label{eq:primedInnerProd}
    \langle f_l^n, g_l^n \rangle_d' = \sum_{l=1}^{N-1} h f_l^n g_l^n + \frac{h}{2}f_0^ng_0^n + \frac{h}{2}f_N^ng_N^n,
\end{equation}
and the more general weighted inner product
\begin{equation}\label{eq:weightedInnerProd}
    \langle f_l^n, g_l^n \rangle_d^{\el, \er} = \sum_{l=1}^{N-1} h f_l^n g_l^n + \frac{\el}{2}hf_0^ng_0^n + \frac{\er}{2}hf_N^ng_N^n,
\end{equation}
which scale the boundary points of the regular inner product. Naturally, if $\el = \er = 1$, Eq. \eqref{eq:weightedInnerProd} reduces to \eqref{eq:primedInnerProd}, and if $\el = \er = 2$, \eqref{eq:weightedInnerProd} reduces to \eqref{eq:discInnerProd}.

\subsection{Summation by Parts}\label{sec:summationByParts}
Extremely useful when performing energy analysis on distributed systems is \textit{summation by parts}, which is the discrete counterpart to integration by parts. Although its application will be only be apparent when actually performing an energy analysis (see fx. Section \ref{sec:energyAnalysisString}) some definitions will be presented here for future reference.

Here, the same functions as in the previous section, $f(x,t)$ and $g(x,t)$ and domain $\D$, will be used. Applying a spatial derivative to $g$, and using the discrete inner product defined in Eq. \eqref{eq:discInnerProd},integration by parts is defined as
\begin{equation}\label{eq:integrationByParts}
    \langle f, \px g \rangle_\D = -\langle \px f, g\rangle_\D + fg|_0^L
\end{equation}
where $fg|_0^L$ describes the boundary terms that appeared in the process. One can observe that the spatial derivative switched function and is now applied to $f$ rather than $g$. Also notice that the sign of the inner product has inverted. 

Defining two (1D) functions $f_l^n$ and $g_l$ defined for $l\in d$ with discrete domain $d=\{0, \hdots, N\}$, two variants of summation by parts are defined as \cite{theBible}\todo{check whether this reference is necessary}
\begin{subequations}\label{eq:summationByParts}
    \begin{align}
        \langle f_l^n, \dxm g_l^n \rangle_d  &= -\langle \dxp f_l^n, g_l^n\rangle_d + f_{N+1}^ng_N^n - f_0^ng_{-1}^n,\label{eq:summationByPartsMinus}\\
        \langle f_l^n, \dxp g_l^n \rangle_d 
        &= -\langle \dxm f_l^n, g_l^n\rangle_d + f_N^ng_{N+1}^n - f_{-1}^ng_0^n.\label{eq:summationByPartsPlus}
    \end{align}
\end{subequations}
A derivation of Eq. \eqref{eq:summationByPartsMinus} is given below. As in the continuous case in Eq. \eqref{eq:integrationByParts}, the process causes the derivative to be applied to the other function and the sign of the resulting inner product changes. Important to note, is that the sign (forward / backward) of the derivative operator has changed. Lastly, discrete boundary terms have appeared and it can be seen that values outside of the defined domain are needed, i.e., $g_{N+1}^n$ and $f_{-1}^n$. These can be accounted for by the boundary conditions imposed on the system (see Section \ref{sec:1DWaveDisc} as an example). 

One could also choose to work with reduced domains after summation by parts. Domains that have one fewer point at the boundaries are defined as $\underline{d} = \{0, \hdots, N-1\}$, 
$\overline{d} = \{1, \hdots, N\}$ and $\underline{\overline{d}} = \{1, \hdots, N-1\}$. The following identities can be shown to hold
\begin{subequations}\label{eq:summationByParts}
    \begin{align}
        \langle f_l^n, \dxm g_l^n \rangle_d  &= -\langle \dxp f_l^n, g_l^n\rangle_{\underline{d}} + f_N^ng_N^n - f_0^ng_{-1}^n,\label{eq:summationByPartsMinusBar}\\
        \langle f_l^n, \dxp g_l^n \rangle_d 
        &= -\langle \dxm f_l^n, g_l^n\rangle_{\overline{d}} + f_N^ng_{N+1}^n - f_0^ng_0^n,\label{eq:summationByPartsPlusBar}
    \end{align}
\end{subequations}
and, using the primed inner product in Eq. \eqref{eq:primedInnerProd},
\begin{subequations}
    \begin{align}
        \langle f_l^n, \dxm g_l^n \rangle_d'  = -\langle \dxp f_l^n, g_l^n \rangle_{\underline{d}}+f_N^n(\mxm g_N^n)-f_0^n(\mxm g_0^n),\label{eq:primedIdentityMinus}\\
        \langle f_l^n, \dxp g_l^n \rangle_d'  = -\langle \dxm f_l^n, g_l^n \rangle_{\overline{d}}+f_N^n(\mxp g_N^n)-f_0^n(\mxp g_0^n),\label{eq:primedIdentityPlus}
    \end{align}
\end{subequations}
%
all of which will prove useful in energy analysis techniques later on. A derivation of \eqref{eq:summationByPartsMinusBar} is given below. 

Finally, one can apply summation by parts twice to get the following identities
\begin{subequations}
    \begin{align}
        \langle f, \dxx g\rangle_d &= \langle \delta_{xx}f, g \rangle_d+f_N\delta_{x+}g_N-g_N\delta_{x+}f_N -f_0\delta_{x-}g_0+g_0\delta_{x-}f_0,\label{eq:summationsummationByPartsTwice}\\
        \langle f, \dxx g\rangle_d &= \langle \delta_{xx}f, g \rangle_{\underline{\overline{d}}}+f_N\dxp g_N-g_N\dxm f_N -f_0\dxm g_0+g_0\dxp f_0,\label{eq:summationByPartsTwiceReduced}\\
        \langle f, \dxx g\rangle_d' &= \langle \dxx f, g\rangle_d' + f_N\dxd g_N - g_N \dxd f_N - f_0 \dxd g_0 + g_0 \dxd f_0\label{eq:summationByPartsTwicePrimed}
    \end{align}
\end{subequations}

\subsubsection{Derivations}
To see why the above identities hold true, it is useful to go through a derivation. As an example, we go through Eqs. \eqref{eq:summationByPartsMinus} and \eqref{eq:summationByPartsMinusBar} as they have the same inner product as a starting point, but yield different results. In the following, $d=\{0, \hdots, N\}$ and $N = 2$ are used. 

Starting with Eq. \eqref{eq:summationByPartsMinus}, suppressing the $n$ superscript for brevity, and using the definition for the discrete inner product in Eq. \eqref{eq:discInnerProd}, we get 
\begin{align*}
    \langle f_l, \dxm g_l \rangle_d &= \sum_{l = 0}^2 h f_l\frac{1}{h}\left(g_l - g_{l-1}\right),\\
    &= f_0g_0 - f_0g_{-1} + f_1g_1 - f_1g_0 + f_2g_2-f_2g_1,\\
    &= g_0(f_0-f_1) - f_0g_{-1} + g_1(f_1-f_2) + g_2(f_2-f_3) + f_3g_2,\\
    &= -g_0(f_1-f_0)- g_1(f_2-f_1) - g_2(f_3-f_2) + f_3g_2 - f_0g_{-1},\\
    &= -\sum_{l=0}^2 h g_l\frac{1}{h}\left(f_{l+1} - f_l\right) + f_3g_2 - f_0g_{-1},\\
    &= -\langle \dxp f_l, g_l\rangle_d + f_3g_2 - f_0g_{-1}.
\end{align*}
As $N=2$, the result is identical to Eq. \eqref{eq:summationByPartsMinus}. 

Using the same inner product as a starting point, identity \eqref{eq:summationByPartsMinusBar} can be proven to hold:
\begin{align*}
    \langle f_l, \dxm g_l \rangle_d &= \sum_{l = 0}^2 h f_l\frac{1}{h}\left(g_l - g_{l-1}\right),\\
    &= f_0g_0 - f_0g_{-1} + f_1g_1 - f_1g_0 + f_2g_2-f_2g_1,\\
    &= -f_0g_{-1} + g_0(f_0-f_1) + g_1(f_1-f_2) + f_2g_2,\\
    &= - g_0(f_1 - f_0) - g_1(f_2-f_1) + f_2g_2 - f_0g_{-1},\\
    &=\sum_{l=0}^1hg_l\frac{1}{h}(f_{l+1}-f_l) + f_2g_2 - f_0g_{-1},\\
    &= -\langle \dxp f_l, g_l\rangle_{\underline{d}} + f_2g_2 - f_0g_{-1}
\end{align*} 
Similar processes can be used to prove the other identities presented in this section.

% Notes on primed plus:
% \begin{equation*}
%     \begin{aligned}
%         \langle f_l, \dxp g_l\rangle_d' &= \sum_{l=1}^{N-1} hf_l\dxp g_l + \frac{h}{2}f_0\dxp g_0 + \frac{h}{2}f_3\dxp g_3\\
%         &= \frac{h}{2}f_0\frac{1}{h}(g_1-g_0)+hf_1\frac{1}{h}(g_2-g_1)+ hf_2\frac{1}{h}(g_3-g_2)+ \frac{h}{2}f_3\frac{1}{h}(g_4-g_3)\\
%         &= \frac{1}{2}f_0g_1-\frac{1}{2}f_0g_0+f_1g_2-f_1g_1+f_2g_3-f_2g_2+\frac{1}{2}f_3g_4-\frac{1}{2}f_3g_3\\
%         &= -\frac{1}{2}f_0g_0 - g_1(f_1-f_0) - \frac{1}{2}f_0g_1-g_2(f_2-f_1)-g_3(f_3-f_2) + \frac{1}{2}f_3g_3 + \frac{1}{2}f_3g_4 \\
%         &= -\sum_{l=1}^N hg_l\dxm f_l - \frac{1}{2}f_0g_0-\frac{1}{2}f_0g_1+\frac{1}{2}f_3g_3+\frac{1}{2}f_3g_4\\
%         &= - \langle \dxm f_l, g_l \rangle_{\underline{d}} + f_3(\mxp g_3) - f_0(\mxp g_0)
%     \end{aligned}
% \end{equation*}

% Alternative domains the following:
% \begin{align*}
%     \langle f, \dxp g \rangle_d &= \sum_{l = 0}^2 h f_l\frac{1}{h}\left(g_{l+1} - g_l\right),\\
%     &= f_0g_1 - f_0g_0 + f_1g_2 - f_1g_1 + f_2g_3-f_2g_2,\\
%     &= -f_0g_0 + g_1(f_0-f_1) + g_2(f_1-f_2) + f_2g_3,\\
%     &= - g_1(f_1 - f_0) - g_2(f_2-f_1) + f_2g_3 - f_0g_0,\\
%     &= -\langle \dxm f, g\rangle_{\overline{d}} + f_2g_3 - f_0g_0
% \end{align*} 

\subsection{Product identities}\label{sec:prodIdentities}
Some useful identities used in this work are
\begin{subequations}
    \begin{align}
        (\dtd \uln)(\dtt \uln) &= \dtp \left(\frac{1}{2}(\dtm \uln)^2\right),\label{eq:prodIdentity1}\\
        (\dtd \uln)\uln &= \dtp \left(\frac{1}{2}\uln e_{t-}\uln\right),\label{eq:prodIdentity2}\\
        (\dtp \uln)(\mtp \uln) &= \dtp \left(\frac{1}{2}(\uln)^2\right),\label{eq:prodIdentity3}\\
        (\dtd \uln)(\mu_{t\cdot}\uln) &= \dtd\left(\frac{1}{2} (\uln)^2\right),\label{eq:prodIdentity4}\\
        \uln e_{t-}\uln &=  (\mtm \uln)^2 - \frac{k^2}{4}(\dtm \uln)^2\label{eq:prodIdentityEnergyStab}
    \end{align}
\end{subequations}
Again, these can be used for spatial derivatives as well by substituting the `$t$' subscripts for `$x$'. Also to 

When an operator is applied to a product of two grid functions, the discrete counterpart of the product rule needs to be used according to
\begin{equation}
    \dtp (\uln\wln) = (\dtp \uln)(\mtp\wln) + (\mtp \uln)(\dtp \wln).
\end{equation}

\section{Frequency Domain Analysis}\label{sec:stabilityAnalysis}
Frequency domain analysis, also called Fourier analysis, is a way to determine various properties of a FD scheme, including conditions for stability. The process is similar to finding stability for digital filters. In essence, a FD scheme can be seen as a complex filter of which its coefficients are defined by physical parameters. \SWcomment[A mass-spring system finds a DSP equivalent in a resonator filter]

\subsubsection{Frequency domain representation and Ansatz}
FD schemes can be analysed by performing a \textit{z-transform}. The z-transform converts a discrete signal into a frequency domain representation, and is extensively used in the field of digital signal processing (DSP) to analyse the behaviour and especially stability of digital filters. To not go too much into detail here, the interested reader is referred to the very comprehensive explanation on the z-transform given in \cite[Ch. 5]{Park2010}. 

If a system is distributed in space, one can perform a spatial Fourier transform on a grid function. Frequency domain analysis in the distributed case is called von Neumann analysis which first appeared in \cite{Strikwerda1989} and is heavily used in \cite{theBible}. The discrete-time z-transform and discrete spatial Fourier transform performed on a 1D grid function are defined as \cite{theBible}
\begin{equation}
    \hat u  = \sum_{n=-\infty}^\infty \uln z^{-n}\qaq \tilde u = \sum_{l=-\infty}^\infty \uln e^{-jl\beta h}
\end{equation}
with complex number $z = e^{sk}$, complex frequency $s=j\omega + \sigma$ (more elaborated on in \ref{sec:modalAnalysis}) and real wavenumber $\beta$. Frequency domain analysis in 2D will be elaborated on in Section \ref{sec:analysis2D}.\todo{check if I should not refer to a subsection}

A shortcut to performing a full frequency domain analysis is to use a test solution, or \textit{ansatz}, and replace the grid functions by their transforms. The grid function for a 1D system can be replaced by an ansatz of the form (1D) \cite{Strikwerda1989}\todo{check reference}
\begin{equation}\label{eq:ansatz}
    u_l^n \ansatz z^n e^{jl\beta h}
\end{equation} 
where ``$\overset{\mathcal{A}}{\Longrightarrow}$'' indicates to replace the grid function with the ansatz (the shortcut to taking the full z-transform and spatial Fourier transform). 

Like in the DSP realm, the power of $z$ indicates a temporal shift, i.e., $z^{-1}$ is a one-sample delay. In a FDTD context, this corresponds to a time shift as seen in Section \ref{sec:FDoperators}. For spatially distributed systems, a shift in $l$ can be interpreted as a phase shift of a frequency with wavenumber $\beta$.\todo{check} See Table \ref{tab:zIdentities} for the frequency domain representation of of grid functions with their temporal and spatial indices shifted in different ways. 


{\renewcommand{\arraystretch}{1.2}

\begin{table}[h]
    \begin{center}
    \begin{tabular}{|c|c|c|}
        \hline
        Grid function & Ansatz & Result\\ \hline
        $u_l^n$ & $z^0 e^{j0\beta h}$ & $1$\\
        $u_l^{n+1}$ & $z^1 e^{j0\beta h}$ & $z$\\
        $u_l^{n-1}$ & $z^{-1} e^{j0\beta h}$ & $z^{-1}$\\
        $u_{l+1}^n$ & $z^0 e^{j1\beta h}$ & $e^{j\beta h}$\\
        $u_{l-1}^n$ & $z^0 e^{j(-1)\beta h}$ & $e^{-j\beta h}$\\
        $u_{l+2}^n$ & $z^0 e^{j2\beta h}$ & $e^{j2\beta h}$\\
        $u_{l-2}^n$ & $z^0 e^{j(-2)\beta h}$ & $e^{-j2\beta h}$\\
        $u_{l+1}^{n-1}$ & $z^{-1} e^{j1\beta h}$ & $z^{-1}e^{j\beta h}$\\
        $u_{l-1}^{n-1}$ & $z^{-1} e^{j(-1)\beta h}$ & $z^{-1}e^{-j\beta h}$\\\hline
    \end{tabular}
    \caption{Frequency-domain representation of a grid function using ansatz \eqref{eq:ansatz} with frequently appearing temporal and spatial shifts.\label{tab:zIdentities}}
    \end{center}
\end{table}
{\renewcommand{\arraystretch}{1}

%In the same way, using ``$\overset{\mathcal{Z}}{\Longrightarrow}$'' and ``$\overset{\mathcal{F}}{\Longrightarrow}$'' to denote a discrete z-transform and Fourier transform respectively,
Using these definitions, the effect of various operators on a grid function can be written in their frequency-domain representation. For systems distributed in space, the following trigonometric identities are extremely useful when performing the analyses \cite[p. 71]{Abramowitz1972}:
\begin{subequations}
    \begin{gather}
        \sin(x) = \frac{e^{jx} - e^{-jx}}{2j}\ \ \Rightarrow \ \ \sin^2(x) %= \frac{e^{j2x} - 2e^{jx-jx}+ e^{-j2x}}{-4} 
        = \frac{e^{j2x} + e^{-j2x}}{-4} + \frac{1}{2},\label{eq:sinIdentity}\\
        \cos(x) = \frac{e^{jx} + e^{-jx}}{2}\ \ \Rightarrow \ \ \cos^2(x) %= \frac{e^{j2x} + 2e^{jx-jx}+ e^{-j2x}}{4} 
        = \frac{e^{j2x} + e^{-j2x}}{4} + \frac{1}{2}.\label{eq:cosIdentity}
    \end{gather}
\end{subequations}
Take for example
\begin{equation*}
    \dxx \uln \ansatz \frac{1}{h^2}\left(e^{j\beta h} - 2 + e^{-j\beta h}\right).
\end{equation*}
Then, using $x = \beta h / 2$, identity \eqref{eq:sinIdentity} can be rewritten to 
\begin{equation*}
    e^{j\beta h} - 2 + e^{-j\beta h} = -4 \sin^2(\beta h / 2),
\end{equation*}
and substituted into the above to get
\begin{equation*}
    \dxx \uln \ansatz -\frac{4}{h^2}\sin^2(\beta h /2).
\end{equation*}
Examples of various temporal operators applied to grid functions in their frequency-domain representation are
\begin{equation}\label{eq:temporalAnsatz}
    \begin{aligned}
    \dtp \uln &\ansatz \frac{1}{k} \left(z - 1\right),& \dtm \uln &\ansatz \frac{1}{k} \left(1 - z^{-1}\right),\\
    \dtd \uln&\ansatz \dfrac{1}{2k} \left(z - z^{-1}\right), &\quad \dtt \uln&\ansatz\frac{1}{k^2} \left(z - 2 + z^{-1}\right)
    \end{aligned}
\end{equation}
and for spatial operators identity \eqref{eq:sinIdentity} can be used to obtain
\begin{subequations}
\begin{align}
    \dxx\uln&\ansatz -\frac{4}{h^2} \sin^2(\beta h / 2), \quad \label{eq:dxxAnsatz}\\
    \dxxxx \uln&\ansatz\frac{16}{h^4} \sin^4(\beta h / 2).\label{eq:dxxxxAnsatz}
\end{align}
\end{subequations}
Frequency domain analysis only works on linear and time-invariant (LTI) systems. Nonlinear \todo{FULL DOC SWEEP: nonlinear, non-linear or non linear} systems can be analysed using energy analysis (see Section \ref{sec:energyAnalysis}).\todo{not talking about nonlinear systems though}   

\subsubsection{Proving stability}
Just as with digital filters, the system is stable when the roots of the polynomial in $z$ are bounded by $1$ (unity)
\begin{equation}\label{eq:boundByUnity}
    |z| \leq 1.
\end{equation} 
In the FDTD context, the frequency-domain representation of a FD scheme results in a \textit{characteristic equation} -- usually a second-order polynomial- in $z$ and needs to satisfy condition \eqref{eq:boundByUnity} for all wave numbers $\beta$.
It can be shown that for a polynomial of the form 
\begin{equation}\label{eq:polynomialForm}
    z^2 + a^{(1)}z + a^{(2)}
\end{equation} 
its roots satisfy condition \eqref{eq:boundByUnity} when it abides the following condition \cite{theBible}
\begin{equation}\label{eq:condition214}
    |a^{(1)}| - 1 \leq a^{(2)} \leq 1.
\end{equation}
If $a^{(2)} = 1$, the simpler condition
\begin{equation}\label{eq:simplerCondition215}
    |a^{(1)}|\leq 2,
\end{equation}
suffices. 

\subsection{Mass-Spring System}\label{sec:massSpringStability}
Recalling the FD scheme of the mass-spring system \eqref{eq:massSpringUpdate}
\begin{equation*}
    M \dtt \uln = -K \uln
\end{equation*} 
a frequency-domain representation can be obtained using the ansatz in \eqref{eq:ansatz} with $l = 0$. Using Table \ref{tab:zIdentities} and Eqs. \eqref{eq:temporalAnsatz} as a reference and substituting the definitions yields
\begin{equation*}
    \frac{M}{k^2}\left(z -2 +z^{-1}\right) = -K.
\end{equation*}
Gathering the terms and moving all to the left-hand side, the characteristic equation for the mass-spring system can be obtained:
\begin{equation}\label{eq:massSpringCharacteristic}
    z - \left(2-\frac{Kk^2}{M} \right) + z^{-1} = 0    
\end{equation}
To begin to prove stability, this equation needs to be written in the form found in \eqref{eq:polynomialForm}. Multiplying all the terms by $z$, and noticing that $a^{(2)} = 1$, we could continue with condition \eqref{eq:simplerCondition215}. However, the scheme used here is a special case where the roots of the characteristic equation can not be identical \cite{theBible}. When this happens, the output of the system will grow linearly and is called ``marginally unstable''. This means that $|a^{(1)}|\neq 1$ and the condition in \eqref{eq:simplerCondition215} becomes $|a^{(1)}|<2$:
\begin{align*}
    \left|-2+\frac{Kk^2}{M}\right| &< 2,\\
    -2 < -2+\frac{Kk^2}{M} &< 2,\\
    0 < \frac{Kk^2}{M} &< 4.
\end{align*}
If only non-zero values are chosen for $K$, $k$ and $M$ they are positive and the first condition is always satisfied. The second condition is then easily solved for $k$ by
\begin{equation}\label{eq:stabilityMK}
    k < 2\sqrt{\frac{M}{K}}
\end{equation}
Recalling that $\omega_0 = \sqrt{K/M}$, Eq \eqref{eq:stabilityMK} can be more compactly written as 
\begin{equation}
    k <\frac{2}{\omega_0}.
\end{equation}

\subsection{1D Wave Equation}\label{sec:vonNeumann1DWave}
This section will derive the stability condition for the 1D wave equation presented in Section \ref{sec:1DWave} using von Neumann analysis.

Recalling the FD scheme in \eqref{eq:1DwaveFDS},
\begin{equation*}
    \dtt \uln = c^2 \dxx \uln,
\end{equation*}
its frequency-domain representation can be obtained using the definitions in Eqs. \eqref{eq:temporalAnsatz} and \eqref{eq:dxxAnsatz}:
\begin{equation}
    \frac{1}{k^2}\left(z - 2 + z^{-1}\right) = -\frac{4c^2}{h^2}\sin^2\left(\beta h / 2\right).
\end{equation}
Also recalling that
\begin{equation*}
    \lambda = \frac{ck}{h},
\end{equation*}
the characteristic equation of the 1D wave equation is
\begin{equation}\label{eq:1dWaveCharacteristic}
    z + \left(4\lambda^2\sin^2(\beta h / 2) -2\right) + z^{-1} = 0.
\end{equation}
% we can now perform von Neumann analysis. Moving all terms to the left-hand side and performing a z-transform and spatial Fourier transform we get:
% \begin{equation}\label{eq:zFDS}
%     z - 2(1-\lambda^2)1 - \lambda^2(e^{j\beta h}+e^{-j\beta h}) + z^{-1} = 0.
% \end{equation}
% Using the identity found in Eq. \eqref{eq:sinIdentity} with $\beta h = 2x \Rightarrow x = \beta h / 2$ we can rewrite \eqref{eq:zFDS} to:
% \begin{equation}
%      z - 2 + 2\lambda^2 +4\lambda^2(\sin^2({\beta h / 2}) - 1/2) - z^{-1} = 0,
% \end{equation}
% and rewriting this yields the characterstic equation shown in Eq. (6.38):
% \begin{equation}\label{eq:characteristic1D}
%      z + 2(2\lambda^2\sin^2(\beta h/2) - 1) + z^{-1} = 0.
% \end{equation}
% The roots are then given by (using $X = \lambda^2\sin^2(\beta h/2)$ for brevity):
% \begin{equation}
% \begin{aligned}\nonumber
%     z_\pm &= \frac{-2(2X-1) \pm \sqrt{4(2X-1)^2 - 4 \cdot 1 \cdot 1}}{2}\\
%     &= 1-2X \pm 1/2 \cdot \sqrt{4(2X-1)^2 - 4}\\
%     &= 1-2X \pm 1/2 \cdot \sqrt{4(4X^2-4X + 1) - 4}\\
%     &= 1-2X \pm 1/2 \cdot \sqrt{(4(4X^2 - 4X) + 4 - 4}\\
%     &= 1-2X \pm 1/2 \cdot \sqrt{4(4X^2 - 4X)}\\
%     &= 1-2X \pm 1/2 \cdot 2 \cdot \sqrt{4X^2 - 4X}\\
%     &= 1-2X \pm \sqrt{(1 - 2X)^2 - 1}\\
% \end{aligned}
% \end{equation}
% which results in the equation for the roots (right below (6.38) in section 6.2.2):
% \begin{equation}
%     z_\pm = 1-2\lambda^2\sin^2(\beta h/2) \pm \sqrt{(1 - 2\lambda^2\sin^2(\beta h/2))^2 - 1}.
% \end{equation}
The scheme is then stable if the roots satisfy condition \eqref{eq:boundByUnity}. As the characteristic equation is of the form in \eqref{eq:polynomialForm} (after multiplication with $z$) with $a^{(2)} = 1$, stability is shown by abiding condition 
\eqref{eq:simplerCondition215} for all $\beta$ and when applied to the characteristic equation \eqref{eq:1dWaveCharacteristic}, it can be seen that
\begin{equation}\nonumber
    \begin{aligned}
        |4\lambda^2\sin^2(\beta h/2) - 2| &\leq 2,\\
        |2\lambda^2\sin^2(\beta h/2) - 1| &\leq 1,\\
        -1 \leq 2\lambda^2\sin^2(\beta h/2) - 1 &\leq 1,\\
        0 \leq 2\lambda^2\sin^2(\beta h/2)&\leq 2,\\
        0\leq \lambda^2\sin^2(\beta h/2) &\leq 1.
    \end{aligned}
\end{equation}
Observing that all terms in $\lambda^2\sin^2(\beta h/2)$ are squared, this term will always be non-negative and therefore always satisfy the first condition. Continuing with the second condition, and knowing that the $\sin^2(\beta h / 2)$-term is bounded by $1$ for all $\beta$, we arrive at the following stability condition:
\begin{equation*}
    \lambda \leq 1.
\end{equation*}
This is the CFL condition given in \eqref{eq:CFL}. To obtain the stability condition in terms of the grid spacing, the definition for $\lambda$ is substituted and written in terms of the grid spacing
\begin{equation}
    h \geq ck,
\end{equation}
which is the stability condition given in Eq. \eqref{eq:1DWaveStabilityCond}.

\section{Energy Analysis}\label{sec:energyAnalysis}

\SWcomment[Energy steps for copying /////]
\subsubsection{Step 1: Obtain $\dtp \h$}

\subsubsection{Step 2: Identify energy types and isolate $\dtp$}

\subsubsection{Step 3: Check units}

\subsubsection{Step 4: Implementation}

\SWcomment[/////]

Energy analysis techniques first appeared in (the first edition of) \cite{Gustafsson2013}\todo{shouldn't I just cite this one then?}

called the energy method

\SWcomment[(also see p. 54 in \cite{theBible}, specifically the section \textbf{Finite precision and round-off error for more references})]


For the 1D wave equation this means to multiply the scheme with $\left(\dtd \uln \right)$


The total energy of the system, or Hamiltonian $\mathfrak{H}$


In implementation it can be very helpful to debug physical models...
One can plot the energy of the system and in a lossless system without external energy input, the rate of change of the total energy should be 0, i.e.,
\begin{equation}\label{eq:unchangedEnergy}
    \delta_{t+}\mathfrak{h} = 0 \quad \Longrightarrow \quad \mathfrak{h}^n = \mathfrak{h}^0.
\end{equation}
.

Although the energy of a lossless system should be unchanged according to Eq. \eqref{eq:unchangedEnergy}, but in a finite precision simulation, ultra slight fluctuations of the energy should be visible due to rounding errors. 

Plotting energy should be within \textit{machine precision}, which mostly is in the range of $10^{-15}$

In this work, the focus of the energy analysis will be in discrete time...

The following steps can be followed to perform a full energy analysis of a FD scheme:

\subsubsection{Step 1: Obtain the rate of change of the total energy $\dtp \h$}
The first step to energy analysis is to take the appropriate \textit{norm} of the scheme (see Eq. \eqref{eq:contInnerProd}), which yields and expression for the rate of change of the energy of the system. Usually, this means to take the inner product of the scheme with $(\dtd \uln)$. See Section \ref{sec:innerProduct} for more details on the inner product.

For the units of the resulting energy balance to add up (also see Step 4), it is useful to perform the analysis on a scheme with all physical parameters written out (so the discretised version of Eq. \eqref{eq:massSpringPDE} rather than Eq. \eqref{eq:massSpringCompact})

\subsubsection{Step 2: Identify different types of energy and obtain the total energy $\h$ by isolating $\dtp$}
The energy of an FD scheme can generally be divided into three different types: the total energy contained within the system, or Hamiltonian $\h$, energy losses through damping $\mathfrak{q}$ and energy input through external forces or excitations $\mathfrak{p}$. For distributed systems, an additional boundary term $\mathfrak{b}$ appears, but vanishes under `regular' (lossless and not energy-storing) boundary conditions. Nearly any energy balance is thus of the form 
\begin{equation}\label{eq:energyForm}
    \dtp \h = \mathfrak{b}-\mathfrak{q} - \mathfrak{p}
\end{equation}
This equation essentially says that the total energy in the system changes due to losses and inputs. For a lossless system without externally supplied energy over the course of the simulation (so initial conditions excluded), the energy should remain unchanged over the course of the simulation, \begin{equation}
    \dtp \h = 0 \quad \Longrightarrow \quad \h^n = \h^0.
\end{equation}

As the eventual interest lies in the total energy of the system $\h$ and not its rate of change, $\dtp$ must be isolated in the definition of $\dtp \h$. In this step, the identities in Section \ref{sec:prodIdentities} will come in handy, as well as summation by parts described in Section \ref{sec:summationByParts} for distributed systems.
 
The Hamiltonian itself can usually be further subdivided into kinetic energy and potential energy, denoted by the symbols $\t$ and $\v$ respectively:
\begin{equation}
    \h = \t + \v
\end{equation}
As a rule of thumb, the definition for kinetic energy contains `velocity squared' (as in the classical-mechanics definition $E_\text{kin}=\tfrac{1}{2}M\dot u$) and the potential energy includes the restoring forces of the system. 

\subsubsection{Step 3: Check units}
To know that the previous steps have been carried out correctly, it is good to check whether the units of the resulting expression for $\h$ is indeed in Joules, or in SI units: kg $\cdot$ m$^2 \cdot $ s$^{-2}$. The other quantities such as energy losses $\mathfrak{q}$ and inputs $\mathfrak{p}$, should be in Joules per second or in SI units: kg $\cdot$ m$^2 \cdot $ s$^{-3}$. Some information about operators and grid functions and how they `add' units to the equation will be given below.

An (1D) inner product (or norm) will `add' one `m' unit due to the $h$ in its definition in \eqref{eq:contInnerProd}. A first-order temporal difference operator will `add' one `s$^{-1}$'-unit (because of the $1/k$) and a first-order spatial difference operator will `add' one `m$^{-1}$'-unit ($1/h$). Along these lines, a second-order time or difference operator will `add' a `s$^{-2}$' ($1/k^2$) or `m$^{-2}$'-unit ($1/h^2$) respectively. It is important to note that the time shift operator ($e_{t-}$) does not influence the units. Finally, if the state $u$ describes a displacement in m, it will `add' this to the equation. If it describes anything else, it will `add' that.

\subsubsection{Step 4: Implement the definitions for energy and debug the FD scheme}
In the end, the definition for the energy can be implemented and used as a check for whether the FD scheme has been implemented correctly.

Subtract $\h^0$

\subsection{Mass-spring system}
Recalling the FD scheme for the simple mass-spring system in Eq. \eqref{eq:massSpringFDS}
\begin{equation*}
    M\dtt \un = -K\un
\end{equation*}
we can start to perform an energy analysis using the five steps described above. 

\subsubsection{Step 1: Obtain $\dtp \h$}
The energy balance of the simple mass-spring system presented in Section \ref{sec:massSpringSystem} can be obtained by first taking the product of scheme \eqref{eq:massSpringFDS} with $(\dtd \un)$:
\begin{equation}\label{eq:rOCmassSpring}
    \dtp \h = M(\dtd\un)(\dtt \un) + K(\dtd\un)(\un) = 0.
\end{equation}
Note that the inner product is not necessary as the system is not distributed.

\subsubsection{Step 2: Identify energy types and isolate $\dtp$}
As there are no losses or externally supplied energy present in the system, all terms are part of the Hamiltonian $\h$. To isolate $\dtp$ from \eqref{eq:rOCmassSpring}, one can use identities \eqref{eq:prodIdentity1} and \eqref{eq:prodIdentity2} to get the following:
\begin{equation}
    \dtp \h = \dtp\left(\frac{M}{2}(\dtm\un)^2 + \frac{K}{2}\un e_{t-}\un\right) = 0,
\end{equation}
and the following definition for $\h$ can be obtained
\begin{equation}\label{eq:energyBalanceMassSpring}
    \h = \frac{M}{2}(\dtm\un)^2 + \frac{K}{2}\un e_{t-}\un = 0.
\end{equation}
This can be rewritten in terms of the kinetic energy $\t$ and potential energy $\v$ according to
\begin{equation}
    \h = \t + \v, \qwiq
    \t = \frac{M}{2}(\dtm\un)^2, \qaq \v = \frac{K}{2}\un e_{t-}\un.
\end{equation} 

\subsubsection{Step 3: Check units}
As mentioned above, the energy $\h$ needs to be in Joules, or kg $\cdot$ m$^2 \cdot$ s$^{-2}$. Taking the terms in Eq. \eqref{eq:energyBalanceMassSpring} one-by-one and writing them in their units results in 
\begin{align*}
    \t = \frac{M}{2}(\dtm\un)^2 &\overset{\text{in units}}{\xrightarrow{\hspace*{1cm}}} \quad\text{kg}\cdot(\text{s}^{-1}\cdot \text{m})^{2}= \text{kg}\cdot\text{m}^2\cdot\text{s}^{-2},\\
    \v = \frac{K}{2}\un e_{t-}\un&\overset{\text{in units}}{\xrightarrow{\hspace*{1cm}}} \quad \text{N} \cdot \text{m}^{-1} \cdot \text{m}\cdot \text{m} = \text{kg}\cdot\text{m}^2\cdot\text{s}^{-2},
\end{align*}
and indeed have the correct units. 

\subsubsection{Step 4: Implementation}
\todo{todo}

\subsection{1D Wave Equation}
Energy analysis could be directly performed on the FD scheme in \eqref{eq:1DwaveFDS}. However, in order for the units of the scheme to add up to energy in Joules, it is better to write out all physical parameters. Taking the definition for the wave speed for the ideal string $c = \sqrt{T/\rho A}$ and multiplying both sides of scheme \eqref{eq:1DwaveFDS} by $\rho A$ yields
\begin{equation}\label{eq:1DWavePhysical}
    \rho A \dtt \uln = T \dxx \uln,
\end{equation}
where $l\in d$ with discrete domain $d\in\{0, \hdots, N\}$ and number of grid points $N+1$. Furthermore, Dirichlet boundary conditions as given in Eq. \eqref{eq:discreteDirichlet} are used. A note on using Neumann boundary conditions is given at the end of this section.

\subsubsection{Step 1: Obtain $\dtp \h$}
Taking an inner product of Eq. \eqref{eq:1DWavePhysical} with $\left( \dtd \uln \right)$ and moving all terms to the left-hand side yields the definition for the rate of change of the Hamiltonian:
\begin{equation}\label{eq:rOC1DWave}
    \dtp \h = \rho A \langle \dtd \uln, \dtt \uln \rangle_d - T \langle \dtd\uln, \dxx \uln\rangle_d = 0.
\end{equation}


\subsubsection{Step 2: Identify energy types and isolate $\dtp$}
As in the case of the mass-spring system in the previous section, there are no losses or externally supplied energy present in the system, and all terms are part of the Hamiltonian $\h$.

To isolate $\dtp$ in Eq. \eqref{eq:rOC1DWave}, the terms have to be rewritten in a way that fits the product identities in Section \ref{sec:prodIdentities}.
Recalling that $\dxx = \dxp\dxm$ we can use summation by parts as described in Eq. \eqref{eq:summationByPartsMinusBar} with $f_l^n \triangleq \dtd \uln$ and $g_l^n \triangleq \dxp \uln$. The second term can then be rewritten to 
%
\begin{equation*}
    - T \langle \dtd\uln, \dxx \uln\rangle_d  = T\langle \dxp(\dtd\uln), \dxp\uln\rangle_{\underline{d}} - \mathfrak{b},
\end{equation*}
where the boundary term
\begin{equation*}
    \mathfrak{b} = T(\dtd u_N^n)(\dxp u_N^n) - T(\dtd u_0^n) (\dxp u_{-1}^n),
\end{equation*}
and reduced domain $\underline{d} = \{0, \hdots, N-1\}$. As Dirichlet boundary conditions are used, the boundary term vanishes as 
\begin{equation*}
    u_0^n = u_N^n = 0 \quad \Longrightarrow \quad \dtd u_0^n = \dtd u_N^n = 0 \quad \Longrightarrow \quad \mathfrak{b} = 0.
\end{equation*}
In other words, if the states of the system at the boundaries are zero, their velocity will also be zero. 
Then, using the discrete inner product in Eq. \eqref{eq:discInnerProd}, Eq. \eqref{eq:rOC1DWave} can be expanded to
\begin{equation}
    \dtp \h = \rho A \sum_{l = 0}^N h(\dtd \uln)(\dtt \uln) + T \sum_{l = 0}^N h(\dtd \dxp \uln)(\dxp \uln)
\end{equation}
Then, using identities \eqref{eq:prodIdentity1} and \eqref{eq:prodIdentity2}, $\dtp$ can be isolated 
\begin{equation}
    \dtp \h = \dtp \left(\frac{\rho A}{2}\lVert\dtm \uln\rVert^2_d + \frac{T}{2}\langle\dxp\uln, e_{t-}\dxp\uln\rangle_{\underline{d}}\right),
\end{equation}
and the definition for the Hamiltonian and the kinetic and potential energy can be found:
\begin{equation}\label{eq:energyBalance1DWave}
    \begin{gathered}
        \h = \t + \v,\\
        \text{with}\quad \t = \frac{\rho A}{2}\lVert\dtm \uln\rVert^2_d, \qaq \v = \frac{T}{2}\langle\dxp\uln, e_{t-}\dxp\uln\rangle_{\underline{d}}.
    \end{gathered}
\end{equation}

\subsubsection{Step 3: Check units}
Writing out the definitions for kinetic and potential energy in Eq. \eqref{eq:energyBalance1DWave} respectively, yields
\begin{align*}
    \t = \frac{\rho A}{2}\lVert\dtm \uln\rVert^2_d \overset{\text{in units}}{\xrightarrow{\hspace*{1cm}}}& \ \text{kg}\cdot \text{m}^{-3} \cdot \text{m}^2\cdot \text{m} \cdot(\text{s}^{-1}\cdot \text{m})^{2}\\
    & = \text{kg}\cdot\text{m}^2\cdot\text{s}^{-2},\\
    \v = \frac{T}{2}\langle\dxp\uln, e_{t-}\dxp\uln\rangle_{\underline{d}}\overset{\text{in units}}{\xrightarrow{\hspace*{1cm}}}& \ \text{N} \cdot \text{m}\cdot (\text{m}^{-1}\cdot\text{m}\cdot \text{m}^{-1}\cdot\text{m}^{-1}\text{m}) \\
    & = \text{kg} \cdot \text{m}^2 \cdot \text{s}^{-2},
\end{align*}
and have the correct units. Notice that an extra `m' unit appears due to the 1D norm and inner product.

\subsubsection{Step 4: Implementation}

\todo{todo}
   
\subsubsection{Neumann boundary conditions}
If Neumann boundary conditions -- as per Eq. \eqref{eq:discreteNeumann} -- are used instead, the primed inner product in Eq. \eqref{eq:primedInnerProd} needs to be used in Step 1. Using the identity in \eqref{eq:primedIdentityMinus}, summation by parts of the second term results in
\begin{equation*}
    - T \langle \dtd\uln, \dxx \uln\rangle_d'  = T\langle \dxp(\dtd\uln), \dxp\uln\rangle_{\underline{d}} - \mathfrak{b},
\end{equation*}
where the boundary term
\begin{align*}
    \mathfrak{b} &= T(\dtd u_N^n)(\mxm\dxp u_N^n) - T(\dtd u_0^n) (\mxm \dxp u_0^n),\\[-1em]
    \xLeftrightarrow{\mystrut\ \text{Eq. \eqref{eq:identity2}}\ } \quad &= T(\dtd u_N^n)(\dxd u_N^n) - T(\dtd u_0^n) (\dxd u_0^n).
\end{align*}
As the Neumann boundary condition states that
\begin{equation*}
    \dxd u_0^n = \dxd u_N^n = 0
\end{equation*}
the boundary term vanishes and the energy balance results in
%
\begin{equation}
    \begin{gathered}
        \h = \t + \v,\\
        \text{with}\quad \t = \frac{\rho A}{2}\Big(\lVert \dtm \uln \rVert'_d\Big)^2, \qaq \frac{T}{2}\langle\dxp\uln, e_{t-}\dxp\uln\rangle_{\underline{d}}.
    \end{gathered}
\end{equation}
\subsection{Stability Analysis using Energy Analysis Techniques}
As opposed to the frequency domain analysis presented in Section \ref{sec:stabilityAnalysis}, energy analysis techniques do not operate in the frequency domain. However, stability can be shown using the techniques presented here. Stability analysis using the energy method might even be considered more powerful than the frequency domain approach, as it can also (to some degree) be used to analyse non-linear and time-varying systems!

One can perform stability analysis using the energy analysis techniques presented here. To arrive at a condition the energy must be \textit{non-negative} ($\h \geq 0$) or, in some cases \textit{positive definite} ($\h > 0$).

\subsubsection{Mass-spring system}
Section \ref{sec:massSpringStability} mentions that the mass-spring system is a special case in that the roots of its characteristic equation can not be identical. When proving stability using energy analysis, this means that the energy of the system needs to be positive definite. It can be shown that an equation of the form 
\begin{equation}\label{eq:quadraticForm}
    x^2 + y^2  + 2axy
\end{equation} 
is positive definite if $|a| < 1$.% If $|a| = 1$, the function is non-negative, but not positive definite.

Equation \eqref{eq:quadraticForm} can be used to prove stability for the mass spring system using the energy balance in Eq. \eqref{eq:energyBalanceMassSpring}. One can easily conclude that $\t$ is positive definite due to non-negative $M$ and the fact that $(\dtm \un)$ is squared. The potential energy $\v$, however, is of indefinite sign. Expanding the operators in Eq. \eqref{eq:energyBalanceMassSpring} yields
\begin{align*}
    \h &= \frac{M}{2k^2}\Big((\un)^2 - 2\un u^{n-1} + (u^{n-1})^2\Big)+ \frac{K}{2}\un u^{n-1},\\
    &= \frac{M}{2k^2}\Big((\un)^2+ (u^{n-1})^2\Big)+ \left(\frac{K}{2}-\frac{M}{k^2}\right)\un u^{n-1}.
\end{align*}
Dividing all terms by $M/2k^2$ this equation is of the form in Eq. \eqref{eq:quadraticForm}:
\begin{equation*}
    \h = (\un)^2+ (u^{n-1})^2+ \left(\frac{Kk^2}{M} - 2\right)\un u^{n-1}.
\end{equation*}
For $\h$ to be positive definite, the following condition must hold
\begin{equation*}
    \left|\frac{Kk^2}{2M} - 1\right| < 1.
\end{equation*}
This can then be written as
\begin{align*}
    -1&<\frac{Kk^2}{2M} - 1<1\\
    0 &< \frac{Kk^2}{2M} < 2
\end{align*}
where, as long as $K$ and $k$ are non-zero, the first inequality is always satisfied. Then the condition solved for $k$ can easily be shown to be
\begin{equation}
    k < 2\sqrt{\frac{M}{K}}
\end{equation}
which is identical to the definition in Eq. \eqref{eq:stabilityMK}.

\subsubsection{1D wave equation}
For the 1D wave equation, one can take the energy balance in Eq. \eqref{eq:energyBalance1DWave} and conclude that $\t$ is non-negative due to the non-negativity of the parameters and $(\dtm \uln)$ being squared. The potential energy, however, is of indefinite sign. One can rewrite $\v$ using identity \eqref{eq:prodIdentityEnergyStab} as
\begin{align*}
    \v &= \frac{T}{2} \langle \dxp \uln, e_{t-}\dxp \uln\rangle_{\underline{d}},\\
    &= \frac{T}{2} \sum_{l=0}^{N-1} h(\dxp \uln)(e_{t-}\dxp\uln),\\
    &= \frac{T}{2} \sum_{l=0}^{N-1} h\left((\mtm \dxp \uln)^2 - \frac{k^2}{4}(\dtm\dxp \uln)^2\right),\\
    &= \frac{T}{2}\left(\lVert\mtm\dxp\uln\rVert_{\underline{d}}^2-\frac{k^2}{4}\lVert\dtm\dxp \uln\rVert_{\underline{d}}^2\right).
\end{align*}
One can then use the following bound for spatial differences \cite{theBible}
\begin{equation}\label{eq:spatialBound}
    \lVert \dxp \uln \rVert_{\underline{d}} \leq \frac{2}{h}\lVert \uln \rVert_d' \leq \frac{2}{h}\lVert \uln \rVert_d,
\end{equation}
to put a condition on $\v$
\begin{align*}
    \v &\geq \frac{T}{2}\left(\lVert\mtm\dxp\uln\rVert_{\underline{d}}^2 - \frac{k^2}{4}\left(\frac{2}{h}\lVert\dtm\uln\rVert_{d}\right)^2\right),\\
    &\geq \frac{T}{2}\left(\lVert\mtm\dxp\uln\rVert_{\underline{d}}^2 - \frac{k^2}{h^2}\lVert\dtm\uln\rVert_{d}^2\right),
\end{align*}
Substituting this condition into the energy balance in Eq. \eqref{eq:energyBalance1DWave} yields
\begin{align*}
    \h = \t + \v &\geq \frac{\rho A}{2}\lVert\dtm \uln\rVert^2_d + \frac{T}{2}\left(\lVert\mtm\dxp\uln\rVert_{\underline{d}}^2 - \frac{k^2}{h^2}\lVert\dtm\uln\rVert_{d}^2\right),\\
    &\geq \left(\frac{\rho A}{2} - \frac{Tk^2}{2h^2}\right)\lVert \dtm \uln\rVert_d^2 + \frac{T}{2}
    \lVert\mtm\dxp \uln\rVert_{\underline{d}}^2,
\end{align*}
which, recalling that $c = \sqrt{T /\rho A}$ and $\lambda = ck/h$ yields
\begin{equation}
    \h = \t + \v \geq \frac{1}{2}\left(1 - \lambda^2\right)\lVert \dtm \uln\rVert_d^2 + \frac{c^2}{2}
    \lVert\mtm\dxp \uln\rVert_{\underline{d}}^2,
\end{equation}
and is non-negative for
\begin{align*}
    1-\lambda^2 &\geq 0,\\
    \lambda &\leq 1.
\end{align*}
This is the same (CFL) condition obtained through von Neumann analysis in Section \ref{sec:vonNeumann1DWave}.

\section{Modal Analysis}
\label{sec:modalAnalysis}
Modes are the resonant frequencies of a system. The amount of modes that a discrete system contains depends on the amount of moving points. A mass-spring system thus has one resonating mode, but a FD scheme of the 1D wave equation with $N = 30$ and Dirichlet boundary conditions will have $29$ modes. This section will show how to analytically obtain the modes of an FD scheme implementing the 1D wave equation. 

We start by using the the matrix form of FD scheme \eqref{eq:1DwaveFDS} from Eq. \eqref{eq:1DwaveMatrix}
\begin{equation*}
    \frac{1}{k^2}\left(\u^{n+1}-2\u^n+\u^{n-1}\right) = c^2 \Dxx\u.
\end{equation*}
Following \cite{theBible} we assume a solution of the form $\u = z^n\boldPhi$ \todo{more explanation, perhaps refer to von neumann analysis in \ref{sec:stabilityAnalysis}}. Substituting this into the above equation yields the characteristic equation
\begin{equation}
    (z - 2 + z^{-1})\boldPhi = c^2k^2\Dxx \boldPhi.
\end{equation}
This is an eigenvalue problem (see Section \ref{sec:eigenValueProblems}) where the $p$\th solution $\boldPhi_p$ may be interpreted as the modal shape of mode $p$. The modal frequencies are the solutions to the following equations:
\begin{gather}
    z_p-2+z_p^{-1} = c^2k^2\text{eig}_p(\Dxx),\nonumber\\
    z_p+(-2-c^2k^2\text{eig}_p(\Dxx))+z_p^{-1}=0.
\end{gather}
\SWcomment[If the CFL condition for the scheme is satisfied, the roots will lie on the unit circle.] Furthermore, we can substitute a test solution $z_p=e^{j\omega_pk}$ with angular frequency of the $p$\th mode $\omega_p$ and solve for these:
\begin{align*}
    e^{j\omega_pk}+e^{-j\omega_pk}-2-c^2k^2\text{eig}_p(\Dxx)&=0,\\
    \frac{e^{j\omega_pk}+e^{-j\omega_pk}}{-4}+\frac{1}{2}+\frac{c^2k^2}{4}\text{eig}_p(\Dxx)&=0,
\end{align*}
and using Eq. \eqref{eq:sinIdentity} we get
\begin{align}
    \sin^2(\omega_pk/2)&+\frac{c^2k^2}{4}\text{eig}_p(\Dxx)=0,\nonumber\\
    \sin(\omega_pk/2)&=\frac{ck}{2}\sqrt{-\text{eig}_p(\Dxx)},\nonumber\\
    \omega_p &= \frac{2}{k}\sin^{-1}\left(\frac{ck}{2}\sqrt{-\text{eig}_p(\Dxx)}\right).
\end{align}

\subsection{One-Step Form}\label{sec:oneStepForm}
For more complicated systems, where the coefficients of $z$ and $z^{-1}$ in the characteristic equation are not identical for example, it is useful to rewrite the update in \textit{one-step form}. Although the eigenfrequency calculation needs to be done on a larger matrix, it allows for a more general and direct way to calculate the modal frequencies and damping coefficients per mode. 

If matrix $\A$ has an inverse, any scheme of the form
\begin{equation}\label{eq:modalForm}
    \A\u^{n+1}=\B\u^n + \C\u^{n-1},
\end{equation}
can be rewritten to
\begin{equation}\label{eq:oneStepForm}
    \underbrace{\begin{bmatrix}
        \u^{n+1}\\
        \u^n
    \end{bmatrix}}_{\w^{n+1}} = 
    \underbrace{\begin{bmatrix}
        \A^{-1}\B & \A^{-1}\C\\
        \I & \mathbf{0}
    \end{bmatrix}}_{\Q}
    \underbrace{\begin{bmatrix}
        \u^n\\
        \u^{n-1}
    \end{bmatrix}}_{\w^n}
\end{equation}
which relates the unknown state of the system to the known state through matrix $\Q$ which encompases the scheme. The sizes of the identity matrix $\I$ and zero matrix $\mathbf{0}$ are the same size as $\A, \B$ and $\C$.

Again, solutions of the form $\w = z^n\boldPhi$ can be assumed \SWcomment[(where $\boldPhi$ is less-trivially connected to the modal shapes)]
\begin{equation}
    z\boldPhi = \Q\boldPhi ,
\end{equation}
which can be solved for the $p$th eigenvalue as
\begin{equation}
    z_p = \text{eig}_p(\Q).
\end{equation}
As the scheme could exhibit damping, the test solution needs to include this and will be $z_p = e^{s_pk}$ with complex frequency $s_p= j\omega_p + \sigma_p$ and damping of the $p$th eigenfrequency $\sigma_p$. Substituting the test solution and solving for $s_p$ yields
\begin{align}
    e^{s_pk} &= \text{eig}_p(\Q),\nonumber\\
    s_p &= \frac{1}{k}\ln \left(\text{eig}_p(\Q)\right).\label{eq:sp}
\end{align}
Solutions for the frequency and damping for the $p$th eigenvalue can then be obtained through
\begin{equation}
    \omega_p = \mathfrak{I}(s_p) \quad \text{and} \quad \sigma_p = \mathfrak{R}(s_p),
\end{equation}
where $\mathfrak{I}(\cdot)$ and $\mathfrak{R}(\cdot)$ denote the ``imaginary part of'' and ``real part of'' respectively. 

As the elements of $\Q$ are real-valued, the solutions $s_p$ in Eq. \eqref{eq:sp} come in complex conjugates (pairs of numbers of which the imaginary part has an opposite sign). For analysis, only the $\mathfrak{I}(s_p)\geq 0$ should be considered as these correspond to non-negative frequencies. 

\section{Dispersion analysis}\label{sec:dispersionAnalysis}
